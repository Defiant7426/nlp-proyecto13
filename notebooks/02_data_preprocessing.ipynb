{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb15d8c0",
   "metadata": {},
   "source": [
    "# Procesamiento de Datos\n",
    "\n",
    "En este notebook vamos a Explorar, implementar y comparar diferentes enfoques de tokenización BPE para el dataset CNN/DailyMail y aplicar tecnicas de preprocesamiento como el trucamiento (como concluimos en el notebook 01) antes de contruir el pipeline de datos final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "252998ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "from collections import Counter, deque\n",
    "from functools import lru_cache\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cd60c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos los datos como hicimos en el notebook 01\n",
    "\n",
    "dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\")\n",
    "\n",
    "train_df = pd.DataFrame(dataset['train'])\n",
    "validation_data_df = pd.DataFrame(dataset['validation'])\n",
    "test_data_df =  pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ae121d",
   "metadata": {},
   "source": [
    "## BPE\n",
    "\n",
    " BPE es un algoritmo que usa la metodologia button-up, empieza con caracteres (bytes) de la secuencia para luego \n",
    " añadir pares frecuentes iterativamente hasta completar la tokenización, esto ayuda a disminuir\n",
    " la cantidad de tokens desconocidos (OOV) y reduce el vocabulario a comparacion que la tokenizacion basada en palabras. Para estas celdas de codigo vamos a \n",
    " utilizar el cuaderno del capitulo 2 bpe-from-scratch.ipynb del repostitorio público de LLMs-from-scratch de \n",
    " Sebastian Raschka.\n",
    "\n",
    " ### Pasos del algoritmo BPE\n",
    "\n",
    " 1. Inicio\n",
    "    1. Empezamos el texto como una secuencia de caracteres individuales \n",
    "    2. El vocabulario inicial son todos los caracteres posibles que se encuentran en el corpus.\n",
    "    3. cada caracter se le asigna un ID (por ejemplo del 0-255)\n",
    " 2. Contar pares mas frecuentes\n",
    "    1. El algoritmo revisa todo el texto y anota el par de caracteres que aparece juntos\n",
    " 3. Añadir el par mas frecuente\n",
    "    1. Teniendo los pares que mas se repiten, entonces le asignamos un nuevo ID \n",
    "    2. Repetir los pasos 1 y 2 continuamente añadiendo los pares mas frecuentes\n",
    "    3. Paramos cuando mas compresion no es posible (cuando ningun par ocurra mas de una vez)\n",
    " 4. Decodificación\n",
    "    1. Usamos la tabla de busqueda para restaurar el texto original haciendo el proceso inverso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ec63b1",
   "metadata": {},
   "source": [
    "### Ejemplo del Algoritmo BPE\n",
    "\n",
    "En el cuardeno de Raschka nos muestra un ejemplo que seria beneficioso tambien estudiarlo, por ejemplo tenemos la secuencia \"the cat in the hat\"\n",
    "\n",
    "#### Iteración 1\n",
    "\n",
    "1. Identifica la frecuencia de pares\n",
    "   1. En la secuencia aparece \"th\" un par de veces \"`th`e cat in `th`e hat\" \n",
    "2. Reemplazamos por un ID\n",
    "   1. Reemplazamos `th` por un ID que no se encuentre en uso, por ejemplo 256\n",
    "   2. Entonces la secuencia se veria asi: \"`<256>`e cat in `<256>`e hat\"\n",
    "   3. Y el vocabulatio se actualizaria asi:\n",
    "\n",
    "        0: ...\n",
    "\n",
    "        1: ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        256: `\"th\"`\n",
    "\n",
    "#### Iteración 2\n",
    "\n",
    "1. Indentifica la frecuencia de pares\n",
    "   1. Ahora de la secuencia anterior aparecece \"<256>e\" un par de veces: \"`<256>e` cat in `<256>e` hat\"\n",
    "2. Reemplazamos por un ID\n",
    "   1. Reemplazamos `<256>e` con un nuevo ID que no se encuentre en uso, por ejemplo 257\n",
    "   2. Entonces la secuencia se veria asi: \"`<257>` cat in `<257>` hat\"\n",
    "   3. El vocabulario se actualizaria asi:\n",
    "        \n",
    "        0: ...\n",
    "\n",
    "        1: ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        256: \"th\"\n",
    "\n",
    "        257: `\"<256>e\"`\n",
    "\n",
    "#### Iteración 3\n",
    "\n",
    "1. Identifica la frecuencia de pares\n",
    "   1. Ahora de la secuencia anterior aparece \"<257> \" un par de veces: \"`<257> `cat in `<257> `hat\"\n",
    "2. Reemplazamos por un ID\n",
    "   1. Reemplazamos `<257> ` por un ID que no se encuentra en uso, por ejemplo 258\n",
    "   2. Entonces la secuencia se veria asi: \"`<258>`cat in `<258>`hat\"\n",
    "   3. El vocabulario se actualizaria asi:\n",
    "\n",
    "        0: ...\n",
    "\n",
    "        1: ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        256: \"th\"\n",
    "\n",
    "        257: \"<256>e\"\n",
    "\n",
    "        258: `\"<257> \"`\n",
    "    \n",
    "#### Iteración 4\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda4ac83",
   "metadata": {},
   "source": [
    "### Implementación del BPE\n",
    "\n",
    "Vamos implementar el codigo educativo que nos proporciona Raschka y lo utilizaremos en nuestro data set de resumenes de noticias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee8bb616",
   "metadata": {},
   "source": [
    "#### Clase BPETokenizerSimple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3663f789",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPETokenizerSimple:\n",
    "    def __init__(self):\n",
    "        # Maps token_id to token_str (e.g., {11246: \"some\"})\n",
    "        self.vocab = {}\n",
    "        # Maps token_str to token_id (e.g., {\"some\": 11246})\n",
    "        self.inverse_vocab = {}\n",
    "        # Dictionary of BPE merges: {(token_id1, token_id2): merged_token_id}\n",
    "        self.bpe_merges = {}\n",
    "\n",
    "        # For the official OpenAI GPT-2 merges, use a rank dict:\n",
    "        #  of form {(string_A, string_B): rank}, where lower rank = higher priority\n",
    "        self.bpe_ranks = {}\n",
    "\n",
    "    def train(self, text, vocab_size, allowed_special={\"<|endoftext|>\"}):\n",
    "        \"\"\"\n",
    "        Train the BPE tokenizer from scratch.\n",
    "\n",
    "        Args:\n",
    "            text (str): The training text.\n",
    "            vocab_size (int): The desired vocabulary size.\n",
    "            allowed_special (set): A set of special tokens to include.\n",
    "        \"\"\"\n",
    "\n",
    "        # Preprocess: Replace spaces with \"Ġ\"\n",
    "        # Note that Ġ is a particularity of the GPT-2 BPE implementation\n",
    "        # E.g., \"Hello world\" might be tokenized as [\"Hello\", \"Ġworld\"]\n",
    "        # (GPT-4 BPE would tokenize it as [\"Hello\", \" world\"])\n",
    "        processed_text = []\n",
    "        for i, char in enumerate(text):\n",
    "            if char == \" \" and i != 0:\n",
    "                processed_text.append(\"Ġ\")\n",
    "            if char != \" \":\n",
    "                processed_text.append(char)\n",
    "        processed_text = \"\".join(processed_text)\n",
    "\n",
    "        # Initialize vocab with unique characters, including \"Ġ\" if present\n",
    "        # Start with the first 256 ASCII characters\n",
    "        unique_chars = [chr(i) for i in range(256)]\n",
    "        unique_chars.extend(\n",
    "            char for char in sorted(set(processed_text))\n",
    "            if char not in unique_chars\n",
    "        )\n",
    "        if \"Ġ\" not in unique_chars:\n",
    "            unique_chars.append(\"Ġ\")\n",
    "\n",
    "        self.vocab = {i: char for i, char in enumerate(unique_chars)}\n",
    "        self.inverse_vocab = {char: i for i, char in self.vocab.items()}\n",
    "\n",
    "        # Add allowed special tokens\n",
    "        if allowed_special:\n",
    "            for token in allowed_special:\n",
    "                if token not in self.inverse_vocab:\n",
    "                    new_id = len(self.vocab)\n",
    "                    self.vocab[new_id] = token\n",
    "                    self.inverse_vocab[token] = new_id\n",
    "\n",
    "        # Tokenize the processed_text into token IDs\n",
    "        token_ids = [self.inverse_vocab[char] for char in processed_text]\n",
    "\n",
    "        # BPE steps 1-3: Repeatedly find and replace frequent pairs\n",
    "        for new_id in range(len(self.vocab), vocab_size):\n",
    "            pair_id = self.find_freq_pair(token_ids, mode=\"most\")\n",
    "            if pair_id is None:\n",
    "                break\n",
    "            token_ids = self.replace_pair(token_ids, pair_id, new_id)\n",
    "            self.bpe_merges[pair_id] = new_id\n",
    "\n",
    "        # Build the vocabulary with merged tokens\n",
    "        for (p0, p1), new_id in self.bpe_merges.items():\n",
    "            merged_token = self.vocab[p0] + self.vocab[p1]\n",
    "            self.vocab[new_id] = merged_token\n",
    "            self.inverse_vocab[merged_token] = new_id\n",
    "\n",
    "    def load_vocab_and_merges_from_openai(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load pre-trained vocabulary and BPE merges from OpenAI's GPT-2 files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocab file (GPT-2 calls it 'encoder.json').\n",
    "            bpe_merges_path (str): Path to the bpe_merges file  (GPT-2 calls it 'vocab.bpe').\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            # Convert loaded vocabulary to correct format\n",
    "            self.vocab = {int(v): k for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {k: int(v) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Handle newline character without adding a new token\n",
    "        if \"\\n\" not in self.inverse_vocab:\n",
    "            # Use an existing token ID as a placeholder for '\\n'\n",
    "            # Preferentially use \"<|endoftext|>\" if available\n",
    "            fallback_token = next((token for token in [\"<|endoftext|>\", \"Ġ\", \"\"] if token in self.inverse_vocab), None)\n",
    "            if fallback_token is not None:\n",
    "                newline_token_id = self.inverse_vocab[fallback_token]\n",
    "            else:\n",
    "                # If no fallback token is available, raise an error\n",
    "                raise KeyError(\"No suitable token found in vocabulary to map '\\\\n'.\")\n",
    "\n",
    "            self.inverse_vocab[\"\\n\"] = newline_token_id\n",
    "            self.vocab[newline_token_id] = \"\\n\"\n",
    "\n",
    "        # Load GPT-2 merges and store them with an assigned \"rank\"\n",
    "        self.bpe_ranks = {}  # reset ranks\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            lines = file.readlines()\n",
    "            if lines and lines[0].startswith(\"#\"):\n",
    "                lines = lines[1:]\n",
    "\n",
    "            rank = 0\n",
    "            for line in lines:\n",
    "                pair = tuple(line.strip().split())\n",
    "                if len(pair) == 2:\n",
    "                    token1, token2 = pair\n",
    "                    # If token1 or token2 not in vocab, skip\n",
    "                    if token1 in self.inverse_vocab and token2 in self.inverse_vocab:\n",
    "                        self.bpe_ranks[(token1, token2)] = rank\n",
    "                        rank += 1\n",
    "                    else:\n",
    "                        print(f\"Skipping pair {pair} as one token is not in the vocabulary.\")\n",
    "\n",
    "    def encode(self, text, allowed_special=None):\n",
    "        \"\"\"\n",
    "        Encode the input text into a list of token IDs, with tiktoken-style handling of special tokens.\n",
    "    \n",
    "        Args:\n",
    "            text (str): The input text to encode.\n",
    "            allowed_special (set or None): Special tokens to allow passthrough. If None, special handling is disabled.\n",
    "    \n",
    "        Returns:\n",
    "            List of token IDs.\n",
    "        \"\"\"\n",
    "        import re\n",
    "    \n",
    "        token_ids = []\n",
    "    \n",
    "        # If special token handling is enabled\n",
    "        if allowed_special is not None and len(allowed_special) > 0:\n",
    "            # Build regex to match allowed special tokens\n",
    "            special_pattern = (\n",
    "                \"(\" + \"|\".join(re.escape(tok) for tok in sorted(allowed_special, key=len, reverse=True)) + \")\"\n",
    "            )\n",
    "    \n",
    "            last_index = 0\n",
    "            for match in re.finditer(special_pattern, text):\n",
    "                prefix = text[last_index:match.start()]\n",
    "                token_ids.extend(self.encode(prefix, allowed_special=None))  # Encode prefix without special handling\n",
    "    \n",
    "                special_token = match.group(0)\n",
    "                if special_token in self.inverse_vocab:\n",
    "                    token_ids.append(self.inverse_vocab[special_token])\n",
    "                else:\n",
    "                    raise ValueError(f\"Special token {special_token} not found in vocabulary.\")\n",
    "                last_index = match.end()\n",
    "    \n",
    "            text = text[last_index:]  # Remaining part to process normally\n",
    "    \n",
    "            # Check if any disallowed special tokens are in the remainder\n",
    "            disallowed = [\n",
    "                tok for tok in self.inverse_vocab\n",
    "                if tok.startswith(\"<|\") and tok.endswith(\"|>\") and tok in text and tok not in allowed_special\n",
    "            ]\n",
    "            if disallowed:\n",
    "                raise ValueError(f\"Disallowed special tokens encountered in text: {disallowed}\")\n",
    "    \n",
    "        # If no special tokens, or remaining text after special token split:\n",
    "        tokens = []\n",
    "        lines = text.split(\"\\n\")\n",
    "        for i, line in enumerate(lines):\n",
    "            if i > 0:\n",
    "                tokens.append(\"\\n\")\n",
    "            words = line.split()\n",
    "            for j, word in enumerate(words):\n",
    "                if j == 0 and i > 0:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "                elif j == 0:\n",
    "                    tokens.append(word)\n",
    "                else:\n",
    "                    tokens.append(\"Ġ\" + word)\n",
    "    \n",
    "        for token in tokens:\n",
    "            if token in self.inverse_vocab:\n",
    "                token_ids.append(self.inverse_vocab[token])\n",
    "            else:\n",
    "                token_ids.extend(self.tokenize_with_bpe(token))\n",
    "    \n",
    "        return token_ids\n",
    "\n",
    "    def tokenize_with_bpe(self, token):\n",
    "        \"\"\"\n",
    "        Tokenize a single token using BPE merges.\n",
    "\n",
    "        Args:\n",
    "            token (str): The token to tokenize.\n",
    "\n",
    "        Returns:\n",
    "            List[int]: The list of token IDs after applying BPE.\n",
    "        \"\"\"\n",
    "        # Tokenize the token into individual characters (as initial token IDs)\n",
    "        token_ids = [self.inverse_vocab.get(char, None) for char in token]\n",
    "        if None in token_ids:\n",
    "            missing_chars = [char for char, tid in zip(token, token_ids) if tid is None]\n",
    "            raise ValueError(f\"Characters not found in vocab: {missing_chars}\")\n",
    "\n",
    "        # If we haven't loaded OpenAI's GPT-2 merges, use my approach\n",
    "        if not self.bpe_ranks:\n",
    "            can_merge = True\n",
    "            while can_merge and len(token_ids) > 1:\n",
    "                can_merge = False\n",
    "                new_tokens = []\n",
    "                i = 0\n",
    "                while i < len(token_ids) - 1:\n",
    "                    pair = (token_ids[i], token_ids[i + 1])\n",
    "                    if pair in self.bpe_merges:\n",
    "                        merged_token_id = self.bpe_merges[pair]\n",
    "                        new_tokens.append(merged_token_id)\n",
    "                        # Uncomment for educational purposes:\n",
    "                        # print(f\"Merged pair {pair} -> {merged_token_id} ('{self.vocab[merged_token_id]}')\")\n",
    "                        i += 2  # Skip the next token as it's merged\n",
    "                        can_merge = True\n",
    "                    else:\n",
    "                        new_tokens.append(token_ids[i])\n",
    "                        i += 1\n",
    "                if i < len(token_ids):\n",
    "                    new_tokens.append(token_ids[i])\n",
    "                token_ids = new_tokens\n",
    "            return token_ids\n",
    "\n",
    "        # Otherwise, do GPT-2-style merging with the ranks:\n",
    "        # 1) Convert token_ids back to string \"symbols\" for each ID\n",
    "        symbols = [self.vocab[id_num] for id_num in token_ids]\n",
    "\n",
    "        # Repeatedly merge all occurrences of the lowest-rank pair\n",
    "        while True:\n",
    "            # Collect all adjacent pairs\n",
    "            pairs = set(zip(symbols, symbols[1:]))\n",
    "            if not pairs:\n",
    "                break\n",
    "\n",
    "            # Find the pair with the best (lowest) rank\n",
    "            min_rank = float(\"inf\")\n",
    "            bigram = None\n",
    "            for p in pairs:\n",
    "                r = self.bpe_ranks.get(p, float(\"inf\"))\n",
    "                if r < min_rank:\n",
    "                    min_rank = r\n",
    "                    bigram = p\n",
    "\n",
    "            # If no valid ranked pair is present, we're done\n",
    "            if bigram is None or bigram not in self.bpe_ranks:\n",
    "                break\n",
    "\n",
    "            # Merge all occurrences of that pair\n",
    "            first, second = bigram\n",
    "            new_symbols = []\n",
    "            i = 0\n",
    "            while i < len(symbols):\n",
    "                # If we see (first, second) at position i, merge them\n",
    "                if i < len(symbols) - 1 and symbols[i] == first and symbols[i+1] == second:\n",
    "                    new_symbols.append(first + second)  # merged symbol\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_symbols.append(symbols[i])\n",
    "                    i += 1\n",
    "            symbols = new_symbols\n",
    "\n",
    "            if len(symbols) == 1:\n",
    "                break\n",
    "\n",
    "        # Finally, convert merged symbols back to IDs\n",
    "        merged_ids = [self.inverse_vocab[sym] for sym in symbols]\n",
    "        return merged_ids\n",
    "\n",
    "    def decode(self, token_ids):\n",
    "        \"\"\"\n",
    "        Decode a list of token IDs back into a string.\n",
    "\n",
    "        Args:\n",
    "            token_ids (List[int]): The list of token IDs to decode.\n",
    "\n",
    "        Returns:\n",
    "            str: The decoded string.\n",
    "        \"\"\"\n",
    "        decoded_string = \"\"\n",
    "        for i, token_id in enumerate(token_ids):\n",
    "            if token_id not in self.vocab:\n",
    "                raise ValueError(f\"Token ID {token_id} not found in vocab.\")\n",
    "            token = self.vocab[token_id]\n",
    "            if token == \"\\n\":\n",
    "                if decoded_string and not decoded_string.endswith(\" \"):\n",
    "                    decoded_string += \" \"  # Add space if not present before a newline\n",
    "                decoded_string += token\n",
    "            elif token.startswith(\"Ġ\"):\n",
    "                decoded_string += \" \" + token[1:]\n",
    "            else:\n",
    "                decoded_string += token\n",
    "        return decoded_string\n",
    "\n",
    "    def save_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Save the vocabulary and BPE merges to JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to save the vocabulary.\n",
    "            bpe_merges_path (str): Path to save the BPE merges.\n",
    "        \"\"\"\n",
    "        # Save vocabulary\n",
    "        with open(vocab_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            json.dump(self.vocab, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "        # Save BPE merges as a list of dictionaries\n",
    "        with open(bpe_merges_path, \"w\", encoding=\"utf-8\") as file:\n",
    "            merges_list = [{\"pair\": list(pair), \"new_id\": new_id}\n",
    "                           for pair, new_id in self.bpe_merges.items()]\n",
    "            json.dump(merges_list, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    def load_vocab_and_merges(self, vocab_path, bpe_merges_path):\n",
    "        \"\"\"\n",
    "        Load the vocabulary and BPE merges from JSON files.\n",
    "\n",
    "        Args:\n",
    "            vocab_path (str): Path to the vocabulary file.\n",
    "            bpe_merges_path (str): Path to the BPE merges file.\n",
    "        \"\"\"\n",
    "        # Load vocabulary\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            loaded_vocab = json.load(file)\n",
    "            self.vocab = {int(k): v for k, v in loaded_vocab.items()}\n",
    "            self.inverse_vocab = {v: int(k) for k, v in loaded_vocab.items()}\n",
    "\n",
    "        # Load BPE merges\n",
    "        with open(bpe_merges_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            merges_list = json.load(file)\n",
    "            for merge in merges_list:\n",
    "                pair = tuple(merge[\"pair\"])\n",
    "                new_id = merge[\"new_id\"]\n",
    "                self.bpe_merges[pair] = new_id\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_special_token_id(self, token):\n",
    "        return self.inverse_vocab.get(token, None)\n",
    "\n",
    "    @staticmethod\n",
    "    def find_freq_pair(token_ids, mode=\"most\"):\n",
    "        pairs = Counter(zip(token_ids, token_ids[1:]))\n",
    "\n",
    "        if not pairs:\n",
    "            return None\n",
    "\n",
    "        if mode == \"most\":\n",
    "            return max(pairs.items(), key=lambda x: x[1])[0]\n",
    "        elif mode == \"least\":\n",
    "            return min(pairs.items(), key=lambda x: x[1])[0]\n",
    "        else:\n",
    "            raise ValueError(\"Invalid mode. Choose 'most' or 'least'.\")\n",
    "\n",
    "    @staticmethod\n",
    "    def replace_pair(token_ids, pair_id, new_id):\n",
    "        dq = deque(token_ids)\n",
    "        replaced = []\n",
    "\n",
    "        while dq:\n",
    "            current = dq.popleft()\n",
    "            if dq and (current, dq[0]) == pair_id:\n",
    "                replaced.append(new_id)\n",
    "                # Remove the 2nd token of the pair, 1st was already removed\n",
    "                dq.popleft()\n",
    "            else:\n",
    "                replaced.append(current)\n",
    "\n",
    "        return replaced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65681a5c",
   "metadata": {},
   "source": [
    "#### Implementando la clase BPETokenizerSimple a nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ceaa0b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WASHINGTON (CNN) -- Doctors removed five small polyps from President Bush's colon on Saturday, and \"none appeared worrisome,\" a White House spokesman said. The polyps were removed and sent to the National Naval Medical Center in Bethesda, Maryland, for routine microscopic examination, spokesman Scott Stanzel said. Results are expected in two to three days. All were small, less than a centimeter [half an inch] in diameter, he said. Bush is in good humor, Stanzel said, and will resume his activities at Camp David. During the procedure Vice President Dick Cheney assumed presidential power. Bush reclaimed presidential power at 9:21 a.m. after about two hours. Doctors used \"monitored anesthesia care,\" Stanzel said, so the president was asleep, but not as deeply unconscious as with a true general anesthetic. He spoke to first lady Laura Bush -- who is in Midland, Texas, celebrating her mother's birthday -- before and after the procedure, Stanzel said. Afterward, the president played with his Scottish terriers, Barney and Miss Beazley, Stanzel said. He planned to have lunch at Camp David and have briefings with National Security Adviser Stephen Hadley and White House Chief of Staff Josh Bolten, and planned to take a bicycle ride Saturday afternoon. Cheney, meanwhile, spent the morning at his home on Maryland's eastern shore, reading and playing with his dogs, Stanzel said. Nothing occurred that required him to take official action as president before Bush reclaimed presidential power. The procedure was supervised by Dr. Richard Tubb, Bush's physician, and conducted by a multidisciplinary team from the National Naval Medical Center in Bethesda, Maryland, the White House said. Bush's last colonoscopy was in June 2002, and no abnormalities were found, White House spokesman Tony Snow said. The president's doctor had recommended a repeat procedure in about five years. A colonoscopy is the most sensitive test for colon cancer, rectal cancer and polyps, small clumps of cells that can become cancerous, according to the Mayo Clinic. Small polyps may be removed during the procedure. Snow said on Friday that Bush had polyps removed during colonoscopies before becoming president. Snow himself is undergoing chemotherapy for cancer that began in his colon and spread to his liver.  Watch Snow talk about Bush's procedure and his own colon cancer » . \"The president wants to encourage everybody to use surveillance,\" Snow said. The American Cancer Society recommends that people without high risk factors or symptoms begin getting screened for signs of colorectal cancer at age 50. E-mail to a friend .\n"
     ]
    }
   ],
   "source": [
    "print(train_df[\"article\"].iloc[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e8f2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Para el ejemplo elegimos el tercer articulo de nuestro dataset de entrenamiento\n",
    "\n",
    "text = train_df[\"article\"].iloc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26d30e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# utilizamos la clase de Raschka para entrenar el algortimo BPE con un token especial para finalizar la secuencia\n",
    "\n",
    "tokenizer = BPETokenizerSimple()\n",
    "tokenizer.train(text, vocab_size=1000, allowed_special={\"<EOS>\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3238cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n"
     ]
    }
   ],
   "source": [
    "# El tamaño del vocabulario\n",
    "\n",
    "print(len(tokenizer.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a24b9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "742\n"
     ]
    }
   ],
   "source": [
    "# La cantidad de veces que se \n",
    "\n",
    "print(len(tokenizer.bpe_merges))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33f1dea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 256, 420, 100, 256, 447, 101, 256, 420, 114, 109, 354, 115, 115, 256, 340, 115, 256, 361, 307, 282, 110, 100, 256, 262, 116, 279, 110, 307, 256, 275, 256, 119, 287, 107, 46]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Bush had five harmless polyps removed and returned to work.\"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb786702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 256, 420, 100, 256, 447, 101, 256, 420, 114, 109, 354, 115, 115, 256, 340, 115, 256, 361, 307, 282, 110, 100, 256, 262, 116, 279, 110, 307, 256, 275, 256, 119, 287, 107, 46, 60, 69, 79, 83, 62]\n"
     ]
    }
   ],
   "source": [
    "# Agregando un token especial EOS = End Of Sequence\n",
    "\n",
    "input_text = \"Bush had five harmless polyps removed and returned to work.<EOS> \"\n",
    "token_ids = tokenizer.encode(input_text)\n",
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d636fd0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 256, 420, 100, 256, 447, 101, 256, 420, 114, 109, 354, 115, 115, 256, 340, 115, 256, 361, 307, 282, 110, 100, 256, 262, 116, 279, 110, 307, 256, 275, 256, 119, 287, 107, 46, 257]\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Bush had five harmless polyps removed and returned to work.<EOS>\"\n",
    "token_ids = tokenizer.encode(input_text, allowed_special={\"<EOS>\"})\n",
    "print(token_ids) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b92aed2b",
   "metadata": {},
   "source": [
    "En la celda anterior notamos que el token especial es asignado al numero 257, es decir que es el primer token asignado luego de enumerar los caracteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0c99be7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numero de caracteres: 64\n",
      "Numero de token IDs: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"Numero de caracteres:\", len(input_text))\n",
    "print(\"Numero de token IDs:\", len(token_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "451202b8",
   "metadata": {},
   "source": [
    "Como vemos, el número de caracteres es diferente al número de tokens. Esto se debe a que el algoritmo agrupa los pares más frecuentes. Por eso, podemos hablar del tamaño de las secuencias en términos de la cantidad de tokens que las componen.\n",
    "\n",
    "En el cuaderno anterior teníamos la disyuntiva de qué hacer cuando teníamos un resumen muy largo comparado con la media. Esa longitud se calculó con respecto al tamaño de las secuencias, pero lo correcto es que se mida en función del tamaño de las secuencias tokenizadas.\n",
    "\n",
    "A partir de allí, podemos truncar las secuencias para quedarnos con una cantidad de tokens que nos interese."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a6978044",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[300, 256, 420, 100, 256, 447, 101, 256, 420, 114, 109, 354, 115, 115, 256, 340, 115, 256, 361, 307, 282, 110, 100, 256, 262, 116, 279, 110, 307, 256, 275, 256, 119, 287, 107, 46, 257]\n"
     ]
    }
   ],
   "source": [
    "print(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a947eb8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 -> Bush\n",
      "256 ->  \n",
      "420 -> ha\n",
      "100 -> d\n",
      "256 ->  \n",
      "447 -> fiv\n",
      "101 -> e\n",
      "256 ->  \n",
      "420 -> ha\n",
      "114 -> r\n",
      "109 -> m\n",
      "354 -> le\n",
      "115 -> s\n",
      "115 -> s\n",
      "256 ->  \n",
      "340 -> polyp\n",
      "115 -> s\n",
      "256 ->  \n",
      "361 -> remov\n",
      "307 -> ed\n",
      "282 ->  a\n",
      "110 -> n\n",
      "100 -> d\n",
      "256 ->  \n",
      "262 -> re\n",
      "116 -> t\n",
      "279 -> ur\n",
      "110 -> n\n",
      "307 -> ed\n",
      "256 ->  \n",
      "275 -> to\n",
      "256 ->  \n",
      "119 -> w\n",
      "287 -> or\n",
      "107 -> k\n",
      "46 -> .\n",
      "257 -> <EOS>\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for token_id in token_ids:\n",
    "    print(f\"{token_id} -> {tokenizer.decode([token_id])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca5cf8",
   "metadata": {},
   "source": [
    "Aqui podemos ver con mas detalle como el algoritmo junta los pares mas frecuentes y le asigna un ID para agregarlo al vocabulario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a93c59bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "16b31424",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a289833a",
   "metadata": {},
   "source": [
    "### Guardar y descargar el tokenizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfb0874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardar el tokenizador entrenado\n",
    "\n",
    "tokenizer.save_vocab_and_merges(vocab_path=\"bpe-tokenizer-raschka/vocab.json\", bpe_merges_path=\"bpe-tokenizer-raschka/bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21ee8af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el tokenizador\n",
    "\n",
    "tokenizer2 = BPETokenizerSimple()\n",
    "tokenizer2.load_vocab_and_merges(vocab_path=\"bpe-tokenizer-raschka/vocab.json\", bpe_merges_path=\"bpe-tokenizer-raschka/bpe_merges.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0b845223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bush had five harmless polyps removed and returned to work.<EOS>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer2.decode(token_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7a5d7c5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This is some text with \\n newline characters.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer2.decode(\n",
    "    tokenizer2.encode(\"This is some text with \\n newline characters.\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b078bc",
   "metadata": {},
   "source": [
    "Raschka no recomienda usar su tokenizador BPE creado desde cero sino mas bien recomienda bibliotecas que se encuentren optimizadas para esa tarea, aqui vamos a presentar dos bibliotecas que es tiktoken de OpenAI que es recomendado por el mismo Raschka y el tokenizador de Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9651468",
   "metadata": {},
   "source": [
    "## Tokenización con tiktoken (OpenAI)\n",
    "\n",
    "Esta biblioteca es recomendado por su rendimiento computacional y es el tokenizador utilizado por los modelos de OpenAI (como GPT-2, GPT-4). Es útil probarlo como un tokenizador BPE pre-entrenado de alto rendimiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b22893",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "\n",
    "enc = tiktoken.get_encoding(\"o200k_base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8c834340",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[83, 8251, 2488, 878, 80401, 0]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.encode(\"tiktoken es genial!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "75a32e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_tokens_from_string(text):\n",
    "    \"\"\"Retorna el numero de tokens en un string.\"\"\"\n",
    "    num_tokens = len(enc.encode(text))\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6b9a89b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tokens_from_string(\"Hello, I'm Luis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "37de7edd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tiktoken es genial!'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "enc.decode([83, 8251, 2488, 878, 80401, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bee343b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b't', b'ikt', b'oken', b' es', b' genial', b'!']"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "[enc.decode_single_token_bytes(token) for token in [83, 8251, 2488, 878, 80401, 0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8dee9a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ca3d7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "058d73b8",
   "metadata": {},
   "source": [
    "# Referencias\n",
    "\n",
    "- Cuarderno de BPE de Raschka: https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/05_bpe-from-scratch/bpe-from-scratch.ipynb\n",
    "- Repositorio de tiktoken: https://github.com/openai/tiktoken "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3fc511",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
