{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f8f404e",
   "metadata": {},
   "source": [
    "# **Configuración del Entrenamiento Seq2Seq con Atención**\n",
    "\n",
    "\n",
    "En este cuaderno, configuraremos todos los componentes necesarios para entrenar nuestro modelo Seq2Seq con atención Bahdanau"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a20546",
   "metadata": {},
   "source": [
    "## **Atencion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15910b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self,enc_hid_dim, dec_hid_dim):\n",
    "        \"\"\"\n",
    "            Constructor de la capa de atencion:\n",
    "            Arg:  \n",
    "                enc_hid_dim (int): Dimension oculta del encoder (BiLSTM).\n",
    "                dec_hid_dim (int): Dimension oculta del decoder (LSTM).\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # Capa lineal para transformar el estado oculto del encoder\n",
    "        self.attn_W_enc = nn.Linear(enc_hid_dim * 2, dec_hid_dim, bias=False)\n",
    "        # Capa lineal para transformar el estado oculto del decoder\n",
    "        self.attn_W_dec = nn.Linear(dec_hid_dim, dec_hid_dim, bias=False)\n",
    "        # Capa lineal para calcular el score final\n",
    "        self.attn_v = nn.Linear(dec_hid_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, decoder_hidden, encoder_outputs):\n",
    "        \"\"\"\n",
    "            Calcula los pesos de atencion y el vector de contexto\n",
    "            Arg:\n",
    "                decoder_hidden (Tensor): Estado oculto del decoder del paso anterior.\n",
    "                encoder_outputs (Tensor): Salidas de todos los pasos de tiempo del encoder.\n",
    "            \n",
    "            Returns:\n",
    "            context_vector (Tensor): Vector de contexto calculado.\n",
    "            attention_weights (Tensor): Pesos de atencion calculados.\n",
    "        \"\"\"\n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "\n",
    "        hidden = decoder_hidden[-1, :, :] # Solo tomamos la ultima capa\n",
    "\n",
    "        decoder_hidden_repeated = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "\n",
    "        energy = torch.tanh(self.attn_W_enc(encoder_outputs) + self.attn_W_dec(decoder_hidden_repeated))\n",
    "\n",
    "        attention_scores = self.attn_v(energy).squeeze(2)\n",
    "\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)\n",
    "\n",
    "        attention_weights_unsqueezed = attention_weights.unsqueeze(1)\n",
    "\n",
    "        context_vector = torch.bmm(attention_weights_unsqueezed, encoder_outputs)\n",
    "\n",
    "        context_vector = context_vector.squeeze(1)\n",
    "\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050eb870",
   "metadata": {},
   "source": [
    "## **model.py con atencion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4740d977",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En este archivo vamos vamos a definir el Encoder, Decoder y Seq2Seq\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim, emb_dim, hidden_dim, n_layers, dropout, pad_idx):\n",
    "        \"\"\"\n",
    "            Constructor del Encoder.\n",
    "            Args: \n",
    "                input_dim(int): tamanio del vocabulario de entrada (fuente o sorce o src).\n",
    "                emb_dim (int): Dimension de los embeddings.\n",
    "                hidden_dim(int): dimension de la capa oculta del LSTM.\n",
    "                n_layers (int): Numero de capas del LSTM\n",
    "                dropout (float): Probabilidad de dropout\n",
    "                pad_idx (idx): Indice del token de padding en el vocabulario\n",
    "        \"\"\"\n",
    "        super().__init__() # Configuraciones internas de nn.Module en el Encoder\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout if n_layers>1 else 0,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "            Procesa la secuencia fuente.\n",
    "            Arg:\n",
    "                src (Tensor): Secuencia de tokens de entrada [batch_size, src_len]\n",
    "            Return:\n",
    "\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        \n",
    "        hidden = hidden.reshape(hidden.size(0), self.n_layers, 2 * self.hidden_dim)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "\n",
    "        cell = cell.permute(1, 0, 2)\n",
    "        cell = cell.reshape(cell.size(0), self.n_layers, 2 * self.hidden_dim)\n",
    "        cell = cell.permute(1, 0, 2)\n",
    "\n",
    "        hidden = torch.tanh(self.fc_hidden(hidden))\n",
    "        cell = torch.tanh(self.fc_cell(cell))\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, emb_dim, enc_hid_dim, dec_hid_dim, n_layers, dropout, pad_idx, attention):\n",
    "        \"\"\"\n",
    "            Inicializador del Decoder.\n",
    "            Args:\n",
    "                output_dim(int): tamanio del vocabulario de salida.\n",
    "                emb_dim(int): dimension de los embeddings.\n",
    "                enc_hid_dim (int): Dimensión oculta del encoder\n",
    "                dec_hid_dim (int): dimension de la capa oculta del LSTM\n",
    "                n_layers(int): Numero de capas del LSTM\n",
    "                dropout (float): Probabilidad de dropout\n",
    "                pad_idx: Indice del token de padding en el vocabulario\n",
    "                attention (Attention): Instancia de la clase Attention.\n",
    "        \"\"\"\n",
    "        super().__init__() # Configuraciones internas del Module.nn en el Decoder\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = dec_hid_dim \n",
    "        self.n_layers = n_layers\n",
    "        self.attention = attention\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim + (enc_hid_dim * 2), dec_hid_dim, n_layers,\n",
    "                           dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(emb_dim + (enc_hid_dim * 2) + dec_hid_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell, encoder_outputs):\n",
    "        \"\"\"\n",
    "            Procesa un paso de la decodificación:\n",
    "            Arg:\n",
    "                input(Tensor): Token de entrada actual [batch size]\n",
    "                hidden(Tensor): Estado oculto del paso anterior [n_layers, batch_size, hidden_dim].\n",
    "                cell(Tensor): Estado de la celda en el paso anterior  [n_layers, batch_size, hidden_dim].\n",
    "            Return:\n",
    "        \"\"\"\n",
    "\n",
    "        input = input.unsqueeze(1) # input = [batch size, 1]\n",
    "        embedded = self.dropout(self.embedding(input)) # embedded = [batch size, 1, emb dim]\n",
    "\n",
    "        context, attention_weights = self.attention(hidden, encoder_outputs)\n",
    "\n",
    "        context = context.unsqueeze(1)\n",
    "\n",
    "        rnn_input = torch.cat((embedded, context), dim=2)\n",
    "\n",
    "        rnn_output, (hidden, cell) = self.rnn(rnn_input, (hidden, cell))\n",
    "\n",
    "        embedded = embedded.squeeze(1)     \n",
    "        context = context.squeeze(1)       \n",
    "        rnn_output = rnn_output.squeeze(1) \n",
    "\n",
    "        fc_input = torch.cat((embedded, context, rnn_output), dim=1)\n",
    "\n",
    "        prediction = self.fc_out(fc_input)\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                encoder(Encoder): instancia del encoder\n",
    "                decoder(Decoder): instancia del decoder\n",
    "                device(torch.device): cpu o cuda \n",
    "        \"\"\"\n",
    "        super().__init__() # Configuraciones internas de nn.Modules en Seq2Seq\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        if hasattr(encoder, 'n_layers') and hasattr(decoder, 'n_layers'):\n",
    "             assert encoder.n_layers == decoder.n_layers, \\\n",
    "                 \"El encoder y decoder deben de tener el mismo numero de capas\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "            Procesa el par de secuencias fuente y objetivo.\n",
    "            Args:\n",
    "                src(Tensor): secuencia fuente [batch_size, src_len].\n",
    "                trg(Tensor): secuencia target [batch_size, trg_len].\n",
    "                teacher_forcing_ratio (float): Probabilidad de usar teacher forcing.\n",
    "            \n",
    "            Return:\n",
    "                output(Tensor): predicciones del decoder.\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len): # Predecimos a partir del segundo token\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell, encoder_outputs)\n",
    "\n",
    "            # Guardamos las predicciones en el tensor de salida\n",
    "            outputs[:, t, :] = output \n",
    "\n",
    "            # Decidir si usar teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # Obtener el token predicho con mayor probabilidad\n",
    "            top1 = output.argmax(1) \n",
    "\n",
    "            # Si es teacher forcing, usar el token real como siguiente input\n",
    "            # Si no, usar el token predicho\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b8ab4b",
   "metadata": {},
   "source": [
    "## **data_loader.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce61dd3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len_article, max_len_highlight, bos_token_id, eos_token_id):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len_article = max_len_article - 2  # Reservar espacio para BOS y EOS\n",
    "        self.max_len_highlight = max_len_highlight - 2\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        article_text = self.dataframe.iloc[idx]['article']\n",
    "        highlight_text = self.dataframe.iloc[idx]['highlights']\n",
    "\n",
    "        # Tokenizar y truncar artículo\n",
    "        self.tokenizer.enable_truncation(max_length=self.max_len_article)\n",
    "        encoded_article = self.tokenizer.encode(article_text)\n",
    "        article_token_ids = encoded_article.ids\n",
    "\n",
    "        # Tokenizar y truncar resumen\n",
    "        self.tokenizer.enable_truncation(max_length=self.max_len_highlight)\n",
    "        encoded_highlight = self.tokenizer.encode(highlight_text)\n",
    "        highlight_token_ids = encoded_highlight.ids\n",
    "\n",
    "        # Añadir tokens BOS/EOS y convertir a tensor\n",
    "        article_tensor = torch.cat(\n",
    "            (torch.tensor([self.bos_token_id]),\n",
    "             torch.tensor(article_token_ids, dtype=torch.long),\n",
    "             torch.tensor([self.eos_token_id]))\n",
    "        )\n",
    "\n",
    "        highlight_tensor = torch.cat(\n",
    "            (torch.tensor([self.bos_token_id]),\n",
    "             torch.tensor(highlight_token_ids, dtype=torch.long),\n",
    "             torch.tensor([self.eos_token_id]))\n",
    "        )\n",
    "\n",
    "        return article_tensor, highlight_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    src_batch_padded = pad_sequence(src_batch, batch_first=True, padding_value=1)  # 1 = PAD_IDX\n",
    "    tgt_batch_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=1)\n",
    "\n",
    "    return src_batch_padded, tgt_batch_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd461a3f",
   "metadata": {},
   "source": [
    "## **Tokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f65495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizador cargado desde: cnn_dailymail_bpe_tokenizer\\tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Cargamos el tokenizer\n",
    "TOKENIZER_DIR = \"cnn_dailymail_bpe_tokenizer\" \n",
    "TOKENIZER_PATH = os.path.join(TOKENIZER_DIR, \"tokenizer.json\")\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print(f\"Tokenizador cargado desde: {TOKENIZER_PATH}\")\n",
    "INPUT_DIM = tokenizer.get_vocab_size()\n",
    "OUTPUT_DIM = tokenizer.get_vocab_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4904dbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = tokenizer.token_to_id(\"<pad>\")\n",
    "BOS_IDX = tokenizer.token_to_id(\"<bos>\")\n",
    "EOS_IDX = tokenizer.token_to_id(\"<eos>\")\n",
    "UNK_IDX = tokenizer.token_to_id(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fdd89db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del Vocabulario (INPUT_DIM/OUTPUT_DIM): 30000\n",
      "Índice de OOV: 0\n",
      "Índice de Padding: 1\n",
      "Índice de Begin of seq.: 2\n",
      "Índice de End of seq.: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño del Vocabulario (INPUT_DIM/OUTPUT_DIM): {INPUT_DIM}\")\n",
    "print(f\"Índice de OOV: {UNK_IDX}\")\n",
    "print(f\"Índice de Padding: {PAD_IDX}\")\n",
    "print(f\"Índice de Begin of seq.: {BOS_IDX}\")\n",
    "print(f\"Índice de End of seq.: {EOS_IDX}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a099ad",
   "metadata": {},
   "source": [
    "## **Hiperparametros**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae141a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n",
      "Batch Size: 16\n",
      "Learning Rate: 0.0005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "ENC_EMB_DIM = 256\n",
    "DEC_EMB_DIM = 256\n",
    "HID_DIM = 512      \n",
    "ENC_HID_DIM = HID_DIM \n",
    "DEC_HID_DIM = HID_DIM\n",
    "N_LAYERS = 2\n",
    "ENC_DROPOUT = 0.3\n",
    "DEC_DROPOUT = 0.3\n",
    "\n",
    "LEARNING_RATE = 0.0005 # tasa de aprendizaje\n",
    "BATCH_SIZE = 16       \n",
    "N_EPOCHS = 10         # Numero de epocas\n",
    "CLIP = 1              # Gradient clipping \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "SEED = 1234\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True \n",
    "\n",
    "MAX_TOKENS_ARTICLE = 800 \n",
    "MAX_TOKENS_HIGHLIGHT = 100  \n",
    "\n",
    "\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}\")\n",
    "print(f\"Learning Rate: {LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d8cf6e",
   "metadata": {},
   "source": [
    "## Instanciar Hiperparametros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "faeee3a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de entrenamiento filtrado cargado.\n",
      "DataFrame de validacion filtrado cargado.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df_filtered = pd.read_parquet(\"data/train_filtered.parquet\") \n",
    "print(\"DataFrame de entrenamiento filtrado cargado.\")\n",
    "\n",
    "validation_df_filtered = pd.read_parquet(\"data/validation_filtered.parquet\") \n",
    "print(\"DataFrame de validacion filtrado cargado.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6762a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_filtered = train_df_filtered[\n",
    "    (train_df_filtered['article'].str.len() <= 1000) & \n",
    "    (train_df_filtered['highlights'].str.len() <= 150)\n",
    "]\n",
    "validation_df_filtered = validation_df_filtered[\n",
    "    (validation_df_filtered['article'].str.len() <= 1000) & \n",
    "    (validation_df_filtered['highlights'].str.len() <= 150)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fed62d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets y DataLoaders creados.\n",
      "  Tamaño Dataset Entrenamiento: 662\n",
      "  Tamaño Dataset Validación: 35\n",
      "  Número batches Entrenamiento: 42\n",
      "  Número batches Validación: 3\n",
      "  Shape src_batch: torch.Size([16, 205])\n",
      "  Shape trg_batch: torch.Size([16, 40])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = SummarizationDataset(\n",
    "    train_df_filtered, tokenizer,\n",
    "    MAX_TOKENS_ARTICLE, MAX_TOKENS_HIGHLIGHT,\n",
    "    BOS_IDX, EOS_IDX\n",
    ")\n",
    "val_dataset = SummarizationDataset(\n",
    "    validation_df_filtered, tokenizer,\n",
    "    MAX_TOKENS_ARTICLE, MAX_TOKENS_HIGHLIGHT,\n",
    "    BOS_IDX, EOS_IDX\n",
    ")\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(f\"Datasets y DataLoaders creados.\")\n",
    "print(f\"  Tamaño Dataset Entrenamiento: {len(train_dataset)}\")\n",
    "print(f\"  Tamaño Dataset Validación: {len(val_dataset)}\")\n",
    "print(f\"  Número batches Entrenamiento: {len(train_dataloader)}\")\n",
    "print(f\"  Número batches Validación: {len(val_dataloader)}\")\n",
    "\n",
    "src_batch_test, trg_batch_test = next(iter(train_dataloader))\n",
    "print(f\"  Shape src_batch: {src_batch_test.shape}\")\n",
    "print(f\"  Shape trg_batch: {trg_batch_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9291c",
   "metadata": {},
   "source": [
    "## **Modelo con atencion**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "becccca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo Seq2Seq con Atención instanciado y movido a device.\n",
      "El modelo tiene 86,215,472 parámetros entrenables.\n"
     ]
    }
   ],
   "source": [
    "attn = Attention(ENC_HID_DIM, DEC_HID_DIM)\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, N_LAYERS, ENC_DROPOUT, PAD_IDX)\n",
    "\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, N_LAYERS, DEC_DROPOUT, PAD_IDX, attn)\n",
    "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "print(\"Modelo Seq2Seq con Atención instanciado y movido a device.\")\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'El modelo tiene {count_parameters(model):,} parámetros entrenables.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca7d26e",
   "metadata": {},
   "source": [
    "## Iniciar Pesos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7fc8ff37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30000, 256, padding_idx=1)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (attention): Attention(\n",
       "      (attn_W_enc): Linear(in_features=1024, out_features=512, bias=False)\n",
       "      (attn_W_dec): Linear(in_features=512, out_features=512, bias=False)\n",
       "      (attn_v): Linear(in_features=512, out_features=1, bias=False)\n",
       "    )\n",
       "    (embedding): Embedding(30000, 256, padding_idx=1)\n",
       "    (rnn): LSTM(1280, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=1792, out_features=30000, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "        else:\n",
    "            nn.init.constant_(param.data, 0)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84dfae9e",
   "metadata": {},
   "source": [
    "## Definimos el Optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "24751860",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\KATANA\\Desktop\\git\\Personal\\nlp-proyecto13\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizador Adam definido con LR=0.0005\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "print(f\"Optimizador Adam definido con LR={LEARNING_RATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e5c41",
   "metadata": {},
   "source": [
    "## **Definimos la Función de Pérdida**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "02682453",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Función de pérdida CrossEntropyLoss definida, ignorando índice 1.\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "print(f\"Función de pérdida CrossEntropyLoss definida, ignorando índice {PAD_IDX}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a18e9",
   "metadata": {},
   "source": [
    "## Funciones Auxiliares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "19bf3a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9b0244",
   "metadata": {},
   "source": [
    "## Implementamos las funciones train y evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f02053fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    \"\"\"\n",
    "    Realiza una época completa de entrenamiento.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): El modelo a entrenar.\n",
    "        iterator (DataLoader): DataLoader para los datos de entrenamiento.\n",
    "        optimizer (optim.Optimizer): Optimizador\n",
    "        criterion (nn.Module): Función de pérdida\n",
    "        clip (float): Valor máximo para el recorte de gradientes\n",
    "\n",
    "    Returns:\n",
    "        float: Perdida (loss) promedio por epoca.\n",
    "    \"\"\"\n",
    "    model.train()  # Modelo en modo entrenamiento\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # Iterampos sobre los batches del DataLoader\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch[0].to(DEVICE) \n",
    "        trg = batch[1].to(DEVICE) \n",
    "\n",
    "        optimizer.zero_grad() # Limpiamos los gradientes\n",
    "\n",
    "        output = model(src, trg, teacher_forcing_ratio=0.5) \n",
    "\n",
    "        output_dim = output.shape[-1] \n",
    "\n",
    "        # Quitamos el token especial <BOS>\n",
    "        output_reshaped = output[:, 1:, :].reshape(-1, output_dim)\n",
    "\n",
    "        # Quitamos el token <BOS> del target y redimensionar\n",
    "        trg_reshaped = trg[:, 1:].reshape(-1)\n",
    "\n",
    "        # Calculamos la perdida\n",
    "        loss = criterion(output_reshaped, trg_reshaped)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Recortamos el gradiente con el clip\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # Actualizar los pesos\n",
    "        optimizer.step()\n",
    "\n",
    "        # Acumulamos las perdidas\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        # Imprimimos el progreso cada 10 batches\n",
    "        if (i + 1) % 10 == 0:\n",
    "            print(f\"  Batch {i+1}/{len(iterator)} - Loss: {loss.item():.4f}\")\n",
    "\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909772c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \"\"\"\n",
    "    Realiza una epoca completa de evaluacion.\n",
    "\n",
    "    Args:\n",
    "        model (Seq2Seq): El modelo a evaluar.\n",
    "        iterator (DataLoader): DataLoader para los datos de validación\n",
    "        criterion (nn.Module): Función de pérdida \n",
    "\n",
    "    Returns:\n",
    "        float: Perdida (loss) promedio por cada epoca.\n",
    "    \"\"\"\n",
    "    model.eval() # Podemos el modelo en modo evaluacion osea que se desactiva el dropout\n",
    "    epoch_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch[0].to(DEVICE)\n",
    "            trg = batch[1].to(DEVICE)\n",
    "\n",
    "            output = model(src, trg, teacher_forcing_ratio=0) # para evaluar se pone el teacher_forcing_ratio en 0\n",
    "\n",
    "            output_dim = output.shape[-1]\n",
    "            output_reshaped = output[:, 1:, :].reshape(-1, output_dim)\n",
    "            trg_reshaped = trg[:, 1:].reshape(-1)\n",
    "\n",
    "            # Calcular la perdida\n",
    "            loss = criterion(output_reshaped, trg_reshaped)\n",
    "\n",
    "            # Acumular la perdida del batch\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "    # Devolver la perdida promedio de cada epoca\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b81531",
   "metadata": {},
   "source": [
    "## Bucle de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8e0b7148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Iniciando Entrenamiento para 10 epocas ---\n",
      "\n",
      "  Batch 10/42 - Loss: 8.5796\n",
      "  Batch 20/42 - Loss: 7.7980\n",
      "  Batch 30/42 - Loss: 7.4291\n",
      "  Batch 40/42 - Loss: 7.4035\n",
      "\t** Modelo guardado en época 1 (Mejor valid loss) **\n",
      "Epoch: 01 | Tiempo: 3m 30s\n",
      "\tTrain Loss: 8.233 | Train PPL: 3763.960\n",
      "\t Val. Loss: 7.732 |  Val. PPL: 2280.050\n",
      "  Batch 10/42 - Loss: 7.0088\n",
      "  Batch 20/42 - Loss: 7.0727\n",
      "  Batch 30/42 - Loss: 6.7724\n",
      "  Batch 40/42 - Loss: 6.8996\n",
      "Epoch: 02 | Tiempo: 3m 29s\n",
      "\tTrain Loss: 6.897 | Train PPL: 988.901\n",
      "\t Val. Loss: 7.803 |  Val. PPL: 2447.268\n",
      "  Batch 10/42 - Loss: 6.4383\n",
      "  Batch 20/42 - Loss: 6.8330\n",
      "  Batch 30/42 - Loss: 6.8197\n",
      "  Batch 40/42 - Loss: 6.8161\n",
      "Epoch: 03 | Tiempo: 3m 33s\n",
      "\tTrain Loss: 6.699 | Train PPL: 811.290\n",
      "\t Val. Loss: 7.787 |  Val. PPL: 2408.011\n",
      "  Batch 10/42 - Loss: 6.6995\n",
      "  Batch 20/42 - Loss: 6.5541\n",
      "  Batch 30/42 - Loss: 6.5245\n",
      "  Batch 40/42 - Loss: 6.7504\n",
      "Epoch: 04 | Tiempo: 3m 29s\n",
      "\tTrain Loss: 6.609 | Train PPL: 741.898\n",
      "\t Val. Loss: 7.920 |  Val. PPL: 2752.278\n",
      "  Batch 10/42 - Loss: 6.5331\n",
      "  Batch 20/42 - Loss: 6.4888\n",
      "  Batch 30/42 - Loss: 6.3449\n",
      "  Batch 40/42 - Loss: 6.6675\n",
      "Epoch: 05 | Tiempo: 3m 30s\n",
      "\tTrain Loss: 6.497 | Train PPL: 663.197\n",
      "\t Val. Loss: 7.952 |  Val. PPL: 2842.078\n",
      "  Batch 10/42 - Loss: 6.3644\n",
      "  Batch 20/42 - Loss: 6.5580\n",
      "  Batch 30/42 - Loss: 6.3582\n",
      "  Batch 40/42 - Loss: 6.4880\n",
      "Epoch: 06 | Tiempo: 3m 30s\n",
      "\tTrain Loss: 6.406 | Train PPL: 605.625\n",
      "\t Val. Loss: 8.010 |  Val. PPL: 3011.336\n",
      "  Batch 10/42 - Loss: 6.0826\n",
      "  Batch 20/42 - Loss: 6.1568\n",
      "  Batch 30/42 - Loss: 6.3065\n",
      "  Batch 40/42 - Loss: 6.5178\n",
      "Epoch: 07 | Tiempo: 3m 31s\n",
      "\tTrain Loss: 6.313 | Train PPL: 551.735\n",
      "\t Val. Loss: 8.004 |  Val. PPL: 2992.000\n",
      "  Batch 10/42 - Loss: 6.1198\n",
      "  Batch 20/42 - Loss: 6.1820\n",
      "  Batch 30/42 - Loss: 6.3982\n",
      "  Batch 40/42 - Loss: 6.3662\n",
      "Epoch: 08 | Tiempo: 3m 29s\n",
      "\tTrain Loss: 6.196 | Train PPL: 490.785\n",
      "\t Val. Loss: 8.126 |  Val. PPL: 3381.166\n",
      "  Batch 10/42 - Loss: 6.1304\n",
      "  Batch 20/42 - Loss: 6.1559\n",
      "  Batch 30/42 - Loss: 5.8532\n",
      "  Batch 40/42 - Loss: 6.1505\n",
      "Epoch: 09 | Tiempo: 3m 29s\n",
      "\tTrain Loss: 6.059 | Train PPL: 427.812\n",
      "\t Val. Loss: 8.142 |  Val. PPL: 3436.230\n",
      "  Batch 10/42 - Loss: 5.8845\n",
      "  Batch 20/42 - Loss: 6.0938\n",
      "  Batch 30/42 - Loss: 6.2706\n",
      "  Batch 40/42 - Loss: 5.9264\n",
      "Epoch: 10 | Tiempo: 3m 29s\n",
      "\tTrain Loss: 5.958 | Train PPL: 386.859\n",
      "\t Val. Loss: 8.242 |  Val. PPL: 3797.614\n",
      "\n",
      "--- Entrenamiento Finalizado ---\n",
      "Mejor Validation Loss alcanzado: 7.732\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math    \n",
    "\n",
    "best_valid_loss = float('inf') # Inicializar con infinito para asegurar que la primera perdida sea mejor\n",
    "\n",
    "print(f\"\\n--- Iniciando Entrenamiento para {N_EPOCHS} epocas ---\")\n",
    "print()\n",
    "\n",
    "\n",
    "    # --- Bucle de Entrenamiento ---\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time() \n",
    "\n",
    "\n",
    "    train_loss = train(model, train_dataloader, optimizer, criterion, CLIP)\n",
    "\n",
    "    valid_loss = evaluate(model, val_dataloader, criterion)\n",
    "\n",
    "    end_time = time.time() \n",
    "\n",
    "    # Calcular duración de la época\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'best-seq2seq-model.pt')\n",
    "        print(f\"\\t** Modelo guardado en época {epoch+1} (Mejor valid loss) **\")\n",
    "\n",
    "    print(f'Epoch: {epoch+1:02} | Tiempo: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
    "\n",
    "print(\"\\n--- Entrenamiento Finalizado ---\")\n",
    "print(f\"Mejor Validation Loss alcanzado: {best_valid_loss:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8510d885",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
