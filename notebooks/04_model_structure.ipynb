{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dab70b6",
   "metadata": {},
   "source": [
    "# Estructura del Modelo Seq2Seq Base\n",
    "\n",
    "En este cuaderno, instanciaremos las clases Encoder, Decoder y Seq2Seq definidas en `src/model.py`. Definiremos los hiperparámetros y pasaremos un lote de datos a través del modelo para verificar que la estructura y las dimensiones son correctas antes de proceder con la implementación de la atención y el entrenamiento."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b982d57",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "071fbdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pegamos las clases de model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self,input_dim, emb_dim, hidden_dim, n_layers, dropout, pad_idx):\n",
    "        \"\"\"\n",
    "            Constructor del Encoder.\n",
    "            Args: \n",
    "                input_dim(int): tamanio del vocabulario de entrada (fuente o sorce o src).\n",
    "                emb_dim (int): Dimension de los embeddings.\n",
    "                hidden_dim(int): dimension de la capa oculta del LSTM.\n",
    "                n_layers (int): Numero de capas del LSTM\n",
    "                dropout (float): Probabilidad de dropout\n",
    "                pad_idx (idx): Indice del token de padding en el vocabulario\n",
    "        \"\"\"\n",
    "        super().__init__() # Configuraciones internas de nn.Module en el Encoder\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout if n_layers>1 else 0,\n",
    "                           bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.fc_hidden = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.fc_cell = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, src):\n",
    "        \"\"\"\n",
    "            Procesa la secuencia fuente.\n",
    "            Arg:\n",
    "                src (Tensor): Secuencia de tokens de entrada [batch_size, src_len]\n",
    "            Return:\n",
    "\n",
    "        \"\"\"\n",
    "        embedded = self.dropout(self.embedding(src))\n",
    "\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "        \n",
    "        hidden = hidden.reshape(hidden.size(0), self.n_layers, 2 * self.hidden_dim)\n",
    "\n",
    "        hidden = hidden.permute(1, 0, 2)\n",
    "\n",
    "        cell = cell.permute(1, 0, 2)\n",
    "        cell = cell.reshape(cell.size(0), self.n_layers, 2 * self.hidden_dim)\n",
    "        cell = cell.permute(1, 0, 2)\n",
    "\n",
    "        hidden = torch.tanh(self.fc_hidden(hidden))\n",
    "        cell = torch.tanh(self.fc_cell(cell))\n",
    "\n",
    "        return outputs, hidden, cell\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self,output_dim, emb_dim, hidden_dim, n_layers, dropout, pad_idx):\n",
    "        \"\"\"\n",
    "            Inicializador del Decoder.\n",
    "            Args:\n",
    "                output_dim(int): tamanio del vocabulario de salida.\n",
    "                emb_dim(int): dimension de los embeddings.\n",
    "                hidden_dim(int): dimension de la capa oculta del LSTM\n",
    "                n_layers(int): Numero de capas del LSTM\n",
    "                dropout (float): Probabilidad de dropout\n",
    "                pad_idx: Indice del token de padding en el vocabulario\n",
    "        \"\"\"\n",
    "        super().__init__() # Configuraciones internas del Module.nn en el Decoder\n",
    "\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, emb_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers,\n",
    "                           dropout=dropout if n_layers > 1 else 0, batch_first=True)\n",
    "        \n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        \"\"\"\n",
    "            Procesa un paso de la decodificación:\n",
    "            Arg:\n",
    "                input(Tensor): Token de entrada actual [batch size]\n",
    "                hidden(Tensor): Estado oculto del paso anterior [n_layers, batch_size, hidden_dim].\n",
    "                cell(Tensor): Estado de la celda en el paso anterior  [n_layers, batch_size, hidden_dim].\n",
    "            Return:\n",
    "        \"\"\"\n",
    "\n",
    "        input = input.unsqueeze(1) # input = [batch size, 1]\n",
    "        embedded = self.dropout(self.embedding(input)) # embedded = [batch size, 1, emb dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "\n",
    "        prediction = self.fc_out(output.squeeze(1))\n",
    "\n",
    "        return prediction, hidden, cell\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \"\"\"\n",
    "            Args:\n",
    "                encoder(Encoder): instancia del encoder\n",
    "                decoder(Decoder): instancia del decoder\n",
    "                device(torch.device): cpu o cuda \n",
    "        \"\"\"\n",
    "        super().__init__() # Configuraciones internas de nn.Modules en Seq2Seq\n",
    "\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "\n",
    "        assert encoder.hidden_dim == decoder.hidden_dim, \\\n",
    "            \"Las dimensiones ocultas del encoder y decoder deben de ser iguales\"\n",
    "        assert encoder.n_layers == decoder.n_layers, \\\n",
    "            \"El encoder y decoder deben de tener el mismo numero de capas\"\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
    "        \"\"\"\n",
    "            Procesa el par de secuencias fuente y objetivo.\n",
    "            Args:\n",
    "                src(Tensor): secuencia fuente [batch_size, src_len].\n",
    "                trg(Tensor): secuencia target [batch_size, trg_len].\n",
    "                teacher_forcing_ratio (float): Probabilidad de usar teacher forcing.\n",
    "            \n",
    "            Return:\n",
    "                output(Tensor): predicciones del decoder.\n",
    "        \"\"\"\n",
    "        batch_size = trg.shape[0]\n",
    "        trg_len = trg.shape[1]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        outputs = torch.zeros(batch_size, trg_len, trg_vocab_size).to(self.device)\n",
    "\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        input = trg[:, 0]\n",
    "\n",
    "        for t in range(1, trg_len): # Predecimos a partir del segundo token\n",
    "\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "\n",
    "            # Guardamos las predicciones en el tensor de salida\n",
    "            outputs[:, t, :] = output \n",
    "\n",
    "            # Decidir si usar teacher forcing\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "\n",
    "            # Obtener el token predicho con mayor probabilidad\n",
    "            top1 = output.argmax(1) \n",
    "\n",
    "            # Si es teacher forcing, usar el token real como siguiente input\n",
    "            # Si no, usar el token predicho\n",
    "            input = trg[:, t] if teacher_force else top1\n",
    "\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21530c1b",
   "metadata": {},
   "source": [
    "## data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86bdd58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len_article, max_len_highlight, bos_token_id, eos_token_id):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len_article = max_len_article - 2  # Reservar espacio para BOS y EOS\n",
    "        self.max_len_highlight = max_len_highlight - 2\n",
    "        self.bos_token_id = bos_token_id\n",
    "        self.eos_token_id = eos_token_id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        article_text = self.dataframe.iloc[idx]['article']\n",
    "        highlight_text = self.dataframe.iloc[idx]['highlights']\n",
    "\n",
    "        # Tokenizar y truncar artículo\n",
    "        self.tokenizer.enable_truncation(max_length=self.max_len_article)\n",
    "        encoded_article = self.tokenizer.encode(article_text)\n",
    "        article_token_ids = encoded_article.ids\n",
    "\n",
    "        # Tokenizar y truncar resumen\n",
    "        self.tokenizer.enable_truncation(max_length=self.max_len_highlight)\n",
    "        encoded_highlight = self.tokenizer.encode(highlight_text)\n",
    "        highlight_token_ids = encoded_highlight.ids\n",
    "\n",
    "        # Añadir tokens BOS/EOS y convertir a tensor\n",
    "        article_tensor = torch.cat(\n",
    "            (torch.tensor([self.bos_token_id]),\n",
    "             torch.tensor(article_token_ids, dtype=torch.long),\n",
    "             torch.tensor([self.eos_token_id]))\n",
    "        )\n",
    "\n",
    "        highlight_tensor = torch.cat(\n",
    "            (torch.tensor([self.bos_token_id]),\n",
    "             torch.tensor(highlight_token_ids, dtype=torch.long),\n",
    "             torch.tensor([self.eos_token_id]))\n",
    "        )\n",
    "\n",
    "        return article_tensor, highlight_tensor\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = [], []\n",
    "    for src_sample, tgt_sample in batch:\n",
    "        src_batch.append(src_sample)\n",
    "        tgt_batch.append(tgt_sample)\n",
    "\n",
    "    src_batch_padded = pad_sequence(src_batch, batch_first=True, padding_value=1)  # 1 = PAD_IDX\n",
    "    tgt_batch_padded = pad_sequence(tgt_batch, batch_first=True, padding_value=1)\n",
    "\n",
    "    return src_batch_padded, tgt_batch_padded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "211c74db",
   "metadata": {},
   "source": [
    "## Cargamos el tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5152e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizador cargado desde: cnn_dailymail_bpe_tokenizer\\tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tokenizers import Tokenizer\n",
    "\n",
    "# Cargamos el tokenizer\n",
    "TOKENIZER_DIR = \"cnn_dailymail_bpe_tokenizer\" \n",
    "TOKENIZER_PATH = os.path.join(TOKENIZER_DIR, \"tokenizer.json\")\n",
    "\n",
    "\n",
    "tokenizer = Tokenizer.from_file(TOKENIZER_PATH)\n",
    "print(f\"Tokenizador cargado desde: {TOKENIZER_PATH}\")\n",
    "INPUT_DIM = tokenizer.get_vocab_size()\n",
    "OUTPUT_DIM = tokenizer.get_vocab_size() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "855616b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAD_IDX = tokenizer.token_to_id(\"<pad>\")\n",
    "BOS_IDX = tokenizer.token_to_id(\"<bos>\")\n",
    "EOS_IDX = tokenizer.token_to_id(\"<eos>\")\n",
    "UNK_IDX = tokenizer.token_to_id(\"<unk>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bb790cd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamaño del Vocabulario (INPUT_DIM/OUTPUT_DIM): 30000\n",
      "Índice de OOV: 0\n",
      "Índice de Padding: 1\n",
      "Índice de Begin of seq.: 2\n",
      "Índice de End of seq.: 3\n"
     ]
    }
   ],
   "source": [
    "print(f\"Tamaño del Vocabulario (INPUT_DIM/OUTPUT_DIM): {INPUT_DIM}\")\n",
    "print(f\"Índice de OOV: {UNK_IDX}\")\n",
    "print(f\"Índice de Padding: {PAD_IDX}\")\n",
    "print(f\"Índice de Begin of seq.: {BOS_IDX}\")\n",
    "print(f\"Índice de End of seq.: {EOS_IDX}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d147f529",
   "metadata": {},
   "source": [
    "## Hiperparametros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "18722b39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hiperparámetros definidos:\n",
      "  ENC_EMB_DIM: 256\n",
      "  DEC_EMB_DIM: 256\n",
      "  HID_DIM: 512\n",
      "  N_LAYERS: 2\n",
      "  ENC_DROPOUT: 0.3\n",
      "  DEC_DROPOUT: 0.3\n",
      "  Device: cpu\n"
     ]
    }
   ],
   "source": [
    "ENC_EMB_DIM = 256  # Dimension Embedding Encoder \n",
    "DEC_EMB_DIM = 256  # Dimension Embedding Decoder\n",
    "HID_DIM = 512      # Dimension Oculta LSTM\n",
    "N_LAYERS = 2       # Numero de capas LSTM \n",
    "ENC_DROPOUT = 0.3  # Dropout Encoder \n",
    "DEC_DROPOUT = 0.3  # Dropout Decoder \n",
    "\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(f\"Hiperparámetros definidos:\")\n",
    "print(f\"  ENC_EMB_DIM: {ENC_EMB_DIM}\")\n",
    "print(f\"  DEC_EMB_DIM: {DEC_EMB_DIM}\")\n",
    "print(f\"  HID_DIM: {HID_DIM}\")\n",
    "print(f\"  N_LAYERS: {N_LAYERS}\")\n",
    "print(f\"  ENC_DROPOUT: {ENC_DROPOUT}\")\n",
    "print(f\"  DEC_DROPOUT: {DEC_DROPOUT}\")\n",
    "print(f\"  Device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3e49867",
   "metadata": {},
   "source": [
    "## Instanciar Hiperparametros del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c49ccf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamos de instancias el Encoder y Decoder\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT, PAD_IDX)\n",
    "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fabb25a",
   "metadata": {},
   "source": [
    "## Instanciar la clase Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "aaaf6bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instancia del modelo Seq2Seq creada y movida al device.\n",
      "El modelo tiene 44,931,376 parámetros entrenables.\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq(enc, dec, DEVICE).to(DEVICE)\n",
    "print(\"Instancia del modelo Seq2Seq creada y movida al device.\")\n",
    "\n",
    "# Función para contar parámetros\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'El modelo tiene {count_parameters(model):,} parámetros entrenables.')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3f690bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Estructura del Modelo:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(30000, 256, padding_idx=1)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "    (fc_hidden): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (fc_cell): Linear(in_features=1024, out_features=512, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(30000, 256, padding_idx=1)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, batch_first=True, dropout=0.3)\n",
       "    (fc_out): Linear(in_features=512, out_features=30000, bias=True)\n",
       "    (dropout): Dropout(p=0.3, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"\\nEstructura del Modelo:\")\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375d3903",
   "metadata": {},
   "source": [
    "## Preparamos un Batch de datos para prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2e4902ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame de entrenamiento filtrado cargado.\n",
      "\n",
      "Primer batch obtenido: 4\n",
      "  Shape src_batch: torch.Size([4, 923])\n",
      "  Shape trg_batch: torch.Size([4, 68])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_TOKENS_ARTICLE = 1200\n",
    "MAX_TOKENS_HIGHLIGHT = 130\n",
    "\n",
    "train_df_filtered = pd.read_parquet(\"data/train_filtered.parquet\") \n",
    "print(\"DataFrame de entrenamiento filtrado cargado.\")\n",
    "\n",
    "# Instanciamos el Dataset\n",
    "temp_train_dataset = SummarizationDataset(\n",
    "    train_df_filtered, tokenizer,\n",
    "    MAX_TOKENS_ARTICLE, MAX_TOKENS_HIGHLIGHT, \n",
    "    BOS_IDX, EOS_IDX\n",
    ")\n",
    "\n",
    "# Instanciamos el Dataloader\n",
    "BATCH_SIZE_TEST = 4 # Usamos un batch de 4 para probar\n",
    "temp_train_dataloader = DataLoader(\n",
    "    dataset=temp_train_dataset,\n",
    "    batch_size=BATCH_SIZE_TEST,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "# Obtenemos un batch\n",
    "src_batch, trg_batch = next(iter(temp_train_dataloader))\n",
    "print(f\"\\nPrimer batch obtenido: {BATCH_SIZE_TEST}\")\n",
    "print(f\"  Shape src_batch: {src_batch.shape}\")\n",
    "print(f\"  Shape trg_batch: {trg_batch.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3cd0ce",
   "metadata": {},
   "source": [
    "## Probamos el Forward Pass del Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f0cf4f5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ejecutando forward pass del modelo...\n",
      "Forward pass completado sin errores.\n",
      "  Shape de la salida (outputs): torch.Size([4, 68, 30000])\n",
      "  Shape esperada: (4, 68, 30000)\n",
      "¡Las dimensiones de salida son correctas!\n"
     ]
    }
   ],
   "source": [
    "model.eval() # Poner el modelo en modo evaluación (desactiva dropout)\n",
    "\n",
    "with torch.no_grad(): # sin gradientes\n",
    "    # Mover datos al mismo dispositivo que el modelo\n",
    "    src_batch = src_batch.to(DEVICE)\n",
    "    trg_batch = trg_batch.to(DEVICE)\n",
    "\n",
    "    print(\"\\nEjecutando forward pass del modelo...\")\n",
    "    try:\n",
    "        outputs = model(src_batch, trg_batch) # teacher_forcing_ratio=0 para probar sin él\n",
    "\n",
    "        print(\"Forward pass completado sin errores.\")\n",
    "        print(f\"  Shape de la salida (outputs): {outputs.shape}\")\n",
    "\n",
    "        expected_shape = (BATCH_SIZE_TEST, trg_batch.shape[1] , OUTPUT_DIM) \n",
    "        print(f\"  Shape esperada: {expected_shape}\")\n",
    "\n",
    "        if outputs.shape == expected_shape:\n",
    "            print(\"¡Las dimensiones de salida son correctas!\")\n",
    "        else:\n",
    "            print(\"¡Advertencia! Las dimensiones de salida NO coinciden con las esperadas.\")\n",
    "            print(f\"Verifica la implementación de Seq2Seq.forward y el uso de batch_first en el Decoder.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError durante el forward pass: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb3364bc",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
